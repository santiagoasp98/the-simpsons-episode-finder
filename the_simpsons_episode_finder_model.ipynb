{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **The Simpsons Episode Finder**"
      ],
      "metadata": {
        "id": "6hmH4nn22BFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Introduction**"
      ],
      "metadata": {
        "id": "tc3kw9VC2ElL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a big fan of *The Simpsons*, I often find myself wanting to rewatch a specific episode based on a vague memory of its plot. However, identifying the exact episode can be complicated. This motivated me to create a web application that can help locate episodes based on that vague memory.\n",
        "\n",
        "While researching how to make this idea a reality, I came across with this dataset on Kaggle: [The Simpsons Dataset](https://www.kaggle.com/datasets/prashant111/the-simpsons-dataset).\n",
        "\n",
        "This dataset seemed like a perfect starting point for training a model. It includes episode metadata, full script lines, and other details, which can be leveraged to build the system."
      ],
      "metadata": {
        "id": "lhymEeHK2HK0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What kind of System is This?**"
      ],
      "metadata": {
        "id": "cZjhvDBr3r6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first thing I asked myself is what type of task I'm trying to solve, and search for references.\n",
        "\n",
        "- **Recommendation system?**: Suggesting episodes based on input descriptions.\n",
        "- **Classification task?**: Predicting the correct episode from a fixed set of classes.\n",
        "- **Text similarity problem?**: Matching user descriptions with episode content.\n",
        "- **Information retrieval?**: Searching through structured text data to find relevand matches.\n",
        "\n",
        "It make more sense to me to approach this task as a text similarity problem. I couldn't find a solution that specifically address this problem, which meant venturing into uncharted territoryâ€”an exciting challenge for a practitioner ðŸ™ƒ."
      ],
      "metadata": {
        "id": "QavM_BwR35DA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Feasibility**"
      ],
      "metadata": {
        "id": "ieNeJye054bL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next question I made was \"can this really be done?\". To determine if the idea was feasible, I identified the following requirements.\n",
        "1. **Episode metadata**: Season and episode numbers, titles, etc.\n",
        "2. **Detailed plot descriptions**: Comprehensive summaries or scripts for each episode.\n",
        "3. **Adequate training data**: A sufficient number of examples to train an effective model.\n",
        "\n",
        "Upon reviewing the dataset, I thought the situation was favorable:\n",
        "- Comprehensive metadata for each episode âœ…\n",
        "- Access to full script lines, which provide more detail than simple summaries âœ…\n",
        "- A reasonable dataset size of ~600 episodes âœ…\n",
        "\n",
        "These factors made the project feasible. However, challenges remain:\n",
        "- Fragmented script lines (individual dialogues rather than summaries).\n",
        "- Need for significan data preprocessing."
      ],
      "metadata": {
        "id": "9kLk-Txz58dP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Importing Libraries**"
      ],
      "metadata": {
        "id": "4KzUZHGx7oUV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2-DeS3msgsd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus    import stopwords\n",
        "from nltk.tokenize  import word_tokenize\n",
        "from nltk.stem      import WordNetLemmatizer\n",
        "from nltk.corpus    import wordnet\n",
        "\n",
        "import re\n",
        "from typing import List, Dict, Any\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Loading the Data**"
      ],
      "metadata": {
        "id": "A2y4TBis7Rm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episodes_path = '/content/drive/MyDrive/ML Projects/The Simpsons/simpsons_episodes.csv'\n",
        "scripts_path = '/content/drive/MyDrive/ML Projects/The Simpsons/simpsons_script_lines.csv'"
      ],
      "metadata": {
        "id": "cmPyqB8_wrxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the scripts data with correct dtypes\n",
        "scripts_df = pd.read_csv(scripts_path, low_memory=False, dtype={\n",
        "    'id': 'int64',\n",
        "    'episode_id': 'int64',\n",
        "    'number': 'int64',\n",
        "    'raw_text': str,\n",
        "    'timestamp_in_ms': str,\n",
        "    'speaking_line': str,\n",
        "    'character_id': str,\n",
        "    'location_id': float,\n",
        "    'raw_character_text': str,\n",
        "    'raw_location_text': str,\n",
        "    'spoken_words': str,\n",
        "    'normalized_text': str,\n",
        "    'word_count': str\n",
        "})\n",
        "\n",
        "# Convert some columns to more appropriate types\n",
        "scripts_df['speaking_line'] = scripts_df['speaking_line'] \\\n",
        "                              .map({'true': True, 'false': False})\n",
        "scripts_df['timestamp_in_ms'] = pd.to_numeric(scripts_df['timestamp_in_ms'],\n",
        "                                              errors='coerce')\n",
        "scripts_df['word_count'] = pd.to_numeric(scripts_df['word_count'],\n",
        "                                         errors='coerce')\n",
        "\n",
        "# For the episodes file, let's keep it simple\n",
        "episodes_df = pd.read_csv(episodes_path, low_memory=False)\n",
        "\n",
        "# Verify the data loaded correctly\n",
        "print(\"Scripts DataFrame shape:\", scripts_df.shape)\n",
        "print(\"Episodes DataFrame shape:\", episodes_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDKhY5vRwlx6",
        "outputId": "28a7cd47-4124-4ded-ee89-c060eddc5a8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scripts DataFrame shape: (158271, 13)\n",
            "Episodes DataFrame shape: (600, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **System Design**"
      ],
      "metadata": {
        "id": "y0SJ9PQY8Dbu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To address this problem, I explored the use of `sentence-transformers` (SBERT) that, according to its documentation:\n",
        "> \"...is the go-to Python module for accessing, using, and training state-of-the-art text and image embedding models. It can be used to compute embeddings using Sentence Transformer models or to calculate similarity scores using Cross-Encoder models.\"\n",
        "\n",
        "Using SBERT, I could do the following:\n",
        "1. **Semantic Embedding Creation**: Generate embeddings for episode content using script lines metadata.\n",
        "2. **Similarity Search**: Implement an efficient similarity search mechanism to match user queries with episodes.\n",
        "3. **Web Application**: Develop a simple interface to take user input and display results."
      ],
      "metadata": {
        "id": "50WmLemP87w9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Some key advantages of this approach**"
      ],
      "metadata": {
        "id": "HLsPfL-d-7zR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Semantic Understanding**: Matches concepts, not just keywords.\n",
        "- **Fast Retrieval**: Efficient search for relevant episodes.\n",
        "- **Confidence Scores**: Indicates the likelihood of correctness.\n",
        "- **Multiple Candidates**: Displays alternative matches if the top result is incorrect.\n",
        "\n",
        "Then, to build the web application, I could simply do:\n",
        "- Save processed episodes and embeddings.\n",
        "- Create a user-friendly web interface with a text input field for queries.\n",
        "- Display results with relevant metadata in an intuitive format."
      ],
      "metadata": {
        "id": "PEAVr8uq-_Iw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Implementation**"
      ],
      "metadata": {
        "id": "8zMr6sFE_oy5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After a lot of trial and error, I identified key considerations for implementation:\n",
        "1. **Preprocessing**:\n",
        "    - **Stopword Removal**: Eliminate common words like \"the\", \"and\", \"is\" to focus on meaningful terms.\n",
        "    - **Lemmatization/Stemming**: Normalize words to their root forms. This is important because episode scripts may contain variations of the same word (e.g., \"run\", \"runs\", \"running\"). By reducing words to their base forms, we can improve the semantic understanding and matching of the text, leading to better results.\n",
        "2. **Model Selection**:\n",
        "    - `all-Min-L6-v2` is fast but struggles with fine-grained distinctions in long texts.\n",
        "    - `all-mpnet-base-v2` offers better embeddings at the cost of slower processing.\n",
        "        - `all-mpnet-base-v2` is a version of MPNet fine-tuned specifically for sentence embeddings, provided by the SentenceTransformers library.\n",
        "3. **Handling Long Texts**:\n",
        "    - Split scripts into smaller chinks, calculate embeddings for each, and aggregate results (e.g., mean embedding).\n",
        "4. **Similarity Metrics**:\n",
        "    - While cosine similarity is straightforward, other options like the dot product may better capture relationships with normalized embeddings.\n",
        "        - Cosine similarity is more sensitive to the direction of the vectors, but not their magnitude. Since we're primarily interested in the semantic closeness of the embeddings, the dot product is a simpler and more effective choice for our use case."
      ],
      "metadata": {
        "id": "VxJ2HMT5_r3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Class `TextPreprocessor`**\n",
        "- `__init__`: Initializes the preprocessor with a lemmatizer and a set of English stopwords.\n",
        "- `get_wordnet_pos(word: str) -> str`: Maps POS tags to WordNet POS tags for better lemmatization.\n",
        "- `preprocess_text(text: str) -> str`: Performs comprehensive text preprocessing, including:\n",
        "    - Lowecasing the text.\n",
        "    - Removing special characters and digits.\n",
        "    - Tokenizing text.\n",
        "    - Removing stopwords and lemmatizing tokens."
      ],
      "metadata": {
        "id": "yv7qQU07Q93P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Class `EpisodeFinder`**\n",
        "- `__init__(episodes_df: pd.DataFrame, scripts_df: pd.DataFrame)`:\n",
        "    - Initializes the finder with preprocessed episode data.\n",
        "    - Loads a `SentenceTransformer` model for generating embeddings.\n",
        "    - Sets a chunk size for splitting long texts.\n",
        "    - Creates embeddings for all episodes.\n",
        "- `_prepare_data(episodes_df: pd.DataFrame, scripts_df: pd.DataFrame) -> pd.DataFrame`:\n",
        "    - Aggregates script lines per episode.\n",
        "    - Merges script data with episode metadata.\n",
        "    - Combines metadata and scripts into a single text field.\n",
        "    - Applies text preprocessing to the combined text.\n",
        "- `_chunk_text(text: str) -> List[str]`: Splits texts into chunks that fit within the model's token limit.\n",
        "- `_create_embeddings()`:\n",
        "    - Generates embeddings for each episode using text chunks.\n",
        "    - Aggregates chunk embeddings by calculating their mean to represent the episode.\n",
        "- `find_episode(description: str, top_k: int = 3) -> List[Dict[str, Any]]`:\n",
        "    - Preprocess the user-provided description.\n",
        "    - Encodes the query into an embedding.\n",
        "    - Calculates similarity scores between the query and episode embeddings.\n",
        "    - Returns the top `k` most similar episodes, including metadata and a similarity score."
      ],
      "metadata": {
        "id": "7dadnKYnUdr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "KHOxRBfcB-_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def get_wordnet_pos(self, word: str) -> str:\n",
        "        \"\"\"Map POS tag to WordNet POS tag\"\"\"\n",
        "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "        tag_dict = {\n",
        "            \"J\": wordnet.ADJ,\n",
        "            \"N\": wordnet.NOUN,\n",
        "            \"V\": wordnet.VERB,\n",
        "            \"R\": wordnet.ADV\n",
        "        }\n",
        "        # (\"J\", \"N\", \"V\", \"R\") correspond to the part-of-speech\n",
        "        # tags used by the WordNet lexical database.\n",
        "        return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "    def preprocess_text(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Apply comprehensive text preprocessing\n",
        "        \"\"\"\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to string and lowercase\n",
        "        text = str(text).lower()\n",
        "\n",
        "        # Remove special characters and digits\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "\n",
        "        # Tokenize\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        # Remove stopwords and lemmatize\n",
        "        # WordNet's POS tags are more fine-grained and tailored for\n",
        "        # linguistic analysis, compared to the more generic NLTK tags.\n",
        "        processed_tokens = []\n",
        "        for token in tokens:\n",
        "            if token not in self.stop_words:\n",
        "                pos = self.get_wordnet_pos(token)\n",
        "                lemma = self.lemmatizer.lemmatize(token, pos)\n",
        "                processed_tokens.append(lemma)\n",
        "\n",
        "        return ' '.join(processed_tokens)"
      ],
      "metadata": {
        "id": "qc1ifn7XtJ86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EpisodeFinder:\n",
        "    def __init__(self, episodes_df: pd.DataFrame, scripts_df: pd.DataFrame):\n",
        "        self.preprocessor = TextPreprocessor()\n",
        "        self.episodes = self._prepare_data(episodes_df, scripts_df)\n",
        "        self.model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "        self.chunk_size = 512  # Maximum tokens for the model\n",
        "        self._create_embeddings()\n",
        "\n",
        "    def _prepare_data(self, episodes_df: pd.DataFrame, scripts_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Prepare and combine episode data with enhanced preprocessing\n",
        "        \"\"\"\n",
        "        # Aggregate script lines per episode\n",
        "        episode_scripts = scripts_df.groupby('episode_id').agg({\n",
        "            'normalized_text': lambda x: ' '.join(str(text) for text in x if pd.notna(text))\n",
        "        }).reset_index()\n",
        "\n",
        "        # Merge with episode metadata\n",
        "        full_episodes = episodes_df.merge(\n",
        "            episode_scripts,\n",
        "            left_on='id',\n",
        "            right_on='episode_id'\n",
        "        )\n",
        "\n",
        "        # Create combined text with metadata\n",
        "        full_episodes['combined_text'] = full_episodes.apply(\n",
        "            lambda row: (\n",
        "                f\"{row['title']} Season {row['season']} \"\n",
        "                f\"Episode {row['number_in_season']} \"\n",
        "                f\"{row['normalized_text']}\"\n",
        "            ),\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        # Preprocess combined text\n",
        "        full_episodes['processed_text'] = full_episodes['combined_text'].apply(\n",
        "            self.preprocessor.preprocess_text\n",
        "        )\n",
        "\n",
        "        return full_episodes\n",
        "\n",
        "    def _chunk_text(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Split text into chunks that fit within model's token limit\n",
        "        \"\"\"\n",
        "        words = text.split()\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "\n",
        "        for word in words:\n",
        "            current_length += len(word) + 1  # +1 for space\n",
        "            if current_length > self.chunk_size:\n",
        "                chunks.append(' '.join(current_chunk))\n",
        "                current_chunk = [word]\n",
        "                current_length = len(word)\n",
        "            else:\n",
        "                current_chunk.append(word)\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def _create_embeddings(self):\n",
        "        \"\"\"\n",
        "        Create embeddings for each episode using chunking\n",
        "        \"\"\"\n",
        "        all_embeddings = []\n",
        "\n",
        "        for text in self.episodes['processed_text']:\n",
        "            chunks = self._chunk_text(text)\n",
        "            chunk_embeddings = self.model.encode(chunks)\n",
        "            # Use mean of chunk embeddings as episode embedding\n",
        "            episode_embedding = np.mean(chunk_embeddings, axis=0)\n",
        "            all_embeddings.append(episode_embedding)\n",
        "\n",
        "        self.episode_embeddings = np.array(all_embeddings)\n",
        "\n",
        "    def find_episode(self, description: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Find most similar episodes to the given description\n",
        "        \"\"\"\n",
        "        # Preprocess the query\n",
        "        processed_query = self.preprocessor.preprocess_text(description)\n",
        "        query_embedding = self.model.encode([processed_query])[0]\n",
        "\n",
        "        # Calculate similarities using dot product\n",
        "        similarities = np.dot(self.episode_embeddings, query_embedding)\n",
        "\n",
        "        # Get top matches\n",
        "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            episode = self.episodes.iloc[idx]\n",
        "            results.append({\n",
        "                'title': episode['title'],\n",
        "                'season': episode['season'],\n",
        "                'episode': episode['number_in_season'],\n",
        "                'similarity_score': float(similarities[idx]),\n",
        "                'description': episode['normalized_text'][:200] + '...'  # Preview\n",
        "            })\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "vptI2eeiuln0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize finder\n",
        "finder = EpisodeFinder(episodes_df, scripts_df)\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "d2v2Bt4HxbaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try it out!"
      ],
      "metadata": {
        "id": "h_x55uGR8gVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = finder.find_episode(\n",
        "    \"Lisa is outperformed in everything she does by a new student\"\n",
        ")\n",
        "for result in results:\n",
        "    print(f\"Season {result['season']} Episode {result['episode']}: {result['title']}\")\n",
        "    print(f\"Similarity Score: {result['similarity_score']:.3f}\")\n",
        "    print(f\"Preview: {result['description']}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbV45zok2KgN",
        "outputId": "26a7f355-09e1-4f48-fe97-929221684286"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Season 6 Episode 2: Lisa's Rival\n",
            "Similarity Score: 0.303\n",
            "Preview: you see marge you see lisa will you keep it down im making a crank phone call to principal skinner uh well as a matter of fact my refrigerator wasnt running you spared me quite a bit of spoilage thank...\n",
            "\n",
            "Season 10 Episode 7: Lisa Gets an \"A\"\n",
            "Similarity Score: 0.302\n",
            "Preview: why you little-- and may we burn in painful and foul-smelling fire forever and ever aaaaa-- mmmennnnonite minister will be giving a guest sermon next sunday go in peace aaaa-- dont make me come up the...\n",
            "\n",
            "Season 15 Episode 3: The President Wore Pearls\n",
            "Similarity Score: 0.300\n",
            "Preview: ooh boy casino night finally theyll teach our kids the dangers of doubling down on a six i really shouldnt be here -- i have a problem with games of chance i played candyland with maggie and ended up ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Saving the model**"
      ],
      "metadata": {
        "id": "Qb9U7QnB8WwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"/content/drive/MyDrive/ML Projects/The Simpsons/saved_models/episode_finder_model\""
      ],
      "metadata": {
        "id": "Jpy3wHXFWxXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "def save_finder(finder, save_path: str):\n",
        "    \"\"\"\n",
        "    Save the EpisodeFinder object, including the SentenceTransformer model and embeddings.\n",
        "    \"\"\"\n",
        "    save_dir = Path(save_path)\n",
        "    save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Save the model\n",
        "    model_path = save_dir / \"sentence_transformer_model\"\n",
        "    finder.model.save(str(model_path))\n",
        "\n",
        "    # Save the other attributes\n",
        "    with open(save_dir / \"finder_data.pkl\", \"wb\") as f:\n",
        "        pickle.dump({\n",
        "            'episodes': finder.episodes,\n",
        "            'episode_embeddings': finder.episode_embeddings,\n",
        "            'chunk_size': finder.chunk_size,\n",
        "        }, f)\n",
        "\n",
        "# Example usage\n",
        "save_finder(finder, save_path)"
      ],
      "metadata": {
        "id": "tHK8DKLbW5bD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_finder(load_path: str):\n",
        "    \"\"\"\n",
        "    Load the EpisodeFinder object from the saved files.\n",
        "    \"\"\"\n",
        "    load_dir = Path(load_path)\n",
        "\n",
        "    # Load the model\n",
        "    model_path = load_dir / \"sentence_transformer_model\"\n",
        "    model = SentenceTransformer(str(model_path))\n",
        "\n",
        "    # Load the other attributes\n",
        "    with open(load_dir / \"finder_data.pkl\", \"rb\") as f:\n",
        "        data = pickle.load(f)\n",
        "\n",
        "    # Recreate the finder object\n",
        "    finder = EpisodeFinder.__new__(EpisodeFinder)  # Create an uninitialized instance\n",
        "    finder.model = model\n",
        "    finder.episodes = data['episodes']\n",
        "    finder.episode_embeddings = data['episode_embeddings']\n",
        "    finder.chunk_size = data['chunk_size']\n",
        "    finder.preprocessor = TextPreprocessor()  # Reinitialize the preprocessor\n",
        "\n",
        "    return finder\n",
        "\n",
        "# Example usage\n",
        "# loaded_finder = load_finder(\"episode_finder_saved\")"
      ],
      "metadata": {
        "id": "KKEjtB_MXWhg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}